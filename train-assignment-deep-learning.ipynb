{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"nteract":{"version":"nteract-front-end@1.0.0"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":30892,"databundleVersionId":2715462,"sourceType":"competition"},{"sourceId":6974438,"sourceType":"datasetVersion","datasetId":4007503}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, time\nimport torch, numpy\nimport cv2\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom PIL import Image\nfrom torchvision.transforms.functional import InterpolationMode\nfrom pathlib import Path\n\ntry:\n    import segmentation_models_pytorch as smp\nexcept:\n    !pip install segmentation-models-pytorch\nimport segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:24:33.714059Z","iopub.execute_input":"2023-11-15T14:24:33.714318Z","iopub.status.idle":"2023-11-15T14:25:00.328290Z","shell.execute_reply.started":"2023-11-15T14:24:33.714293Z","shell.execute_reply":"2023-11-15T14:25:00.327333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(3)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:00.329947Z","iopub.execute_input":"2023-11-15T14:25:00.330252Z","iopub.status.idle":"2023-11-15T14:25:00.410294Z","shell.execute_reply.started":"2023-11-15T14:25:00.330222Z","shell.execute_reply":"2023-11-15T14:25:00.409546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    import wandb\nexcept:\n    !pip install wandb\nimport wandb\n\nwandb.login(key = \"280aa3837eb27ece3c32ed8e27e3e233d0afdc9c\")\nwandb.init(project=\"Unet_Deep_Learing_Assignment\")","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:00.411388Z","iopub.execute_input":"2023-11-15T14:25:00.411753Z","iopub.status.idle":"2023-11-15T14:25:00.415792Z","shell.execute_reply.started":"2023-11-15T14:25:00.411718Z","shell.execute_reply":"2023-11-15T14:25:00.414776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"config = {\n    ### Path bộ data được sample\n    \"sample_txt\": None, # \"/kaggle/working/sample_150_data.txt\" # nếu train cả bộ data thì để None\n    \"number_of_sample_data\": 150,\n    \n    ### Path của folder\n    \"train_folder\": \"/kaggle/input/bkai-igh-neopolyp/train/train\",\n    \"train_gt_folder\": \"/kaggle/input/bkai-igh-neopolyp/train_gt/train_gt\",\n    \n    ### Thông số chia tập train - val\n    \"train_percent\": 0.8,\n    \n    ### Thông số cho quá trình train\n    \"epochs\" : 50,\n    \"batch_size\" : 8,\n    \"learning_rate\" : 0.0001,\n    \n    ### Load và Save model\n    \"load_model\": None,\n    \"save_folder\": \"/kaggle/working/save\"\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:00.418064Z","iopub.execute_input":"2023-11-15T14:25:00.418336Z","iopub.status.idle":"2023-11-15T14:25:00.425860Z","shell.execute_reply.started":"2023-11-15T14:25:00.418312Z","shell.execute_reply":"2023-11-15T14:25:00.425098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **0. Sample ra bộ data nhỏ hơn để train và đánh giá thử**\nTa sẽ lấy random ra 1 bộ data nhỏ hơn (150 samples) để train và đánh giá thử cho các model.  \nSau đó sẽ chọn ra model tốt nhất rồi train bằng cả bộ data gốc.","metadata":{}},{"cell_type":"code","source":"if config[\"sample_txt\"] != None:\n    numpy.random.seed(3)\n    sample_files = numpy.random.choice(a = os.listdir(config[\"train_folder\"]), size = config[\"number_of_sample_data\"])\n    with open(config[\"sample_txt\"], \"w\") as f:\n        for file_name in sample_files:\n            f.write(f\"{file_name}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:00.426915Z","iopub.execute_input":"2023-11-15T14:25:00.427169Z","iopub.status.idle":"2023-11-15T14:25:00.478187Z","shell.execute_reply.started":"2023-11-15T14:25:00.427146Z","shell.execute_reply":"2023-11-15T14:25:00.477485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1 .Preprocess data. Dataset và DataLoader**","metadata":{}},{"cell_type":"code","source":"def createPreprocessTransform():\n    return transforms.Compose([\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.5),\n        transforms.Resize((800, 1280)),\n        transforms.ToTensor(),\n    ])","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:00.479134Z","iopub.execute_input":"2023-11-15T14:25:00.479405Z","iopub.status.idle":"2023-11-15T14:25:00.484694Z","shell.execute_reply.started":"2023-11-15T14:25:00.479381Z","shell.execute_reply":"2023-11-15T14:25:00.483815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NeopolypDataset(Dataset):\n    def __init__(self, \n                 preprocess_transform: transforms.Compose,\n                 sample_data_txt: str = config[\"sample_txt\"],\n                 train_folder: str = config[\"train_folder\"],\n                 train_gt_folder: str = config[\"train_gt_folder\"]\n                ):\n        ### Lấy tên các file\n        files = None\n        if sample_data_txt != None:\n            with open(sample_data_txt, \"r\") as f:\n                files = f.read().splitlines()\n        else:\n            files = os.listdir(train_folder)\n        ### Tạo path cho data và label\n        self.data_paths = [f\"{train_folder}/{file_name}\" for file_name in files]\n        self.label_paths = [f\"{train_gt_folder}/{file_name}\" for file_name in files]\n        self.preprocess_transform = preprocess_transform\n        \n    def __len__(self):\n        return len(self.data_paths)\n    \n    def __getitem__(self, index):\n        ### Đọc ảnh .jpeg -> PIL\n        data_pil = Image.open(self.data_paths[index])\n        label_pil = Image.open(self.label_paths[index])\n        \n        ### Preprocessing ảnh, bao gồm cả việc toTensor\n        data = self.preprocess_transform(data_pil)\n        label = self.preprocess_transform(label_pil)\n        \n        ### Normalize:\n        data = data/255\n        label = label/255\n        \n        ### Convert label từ ảnh RGB -> ma trận index class {0, 1, 2}\n        label = torch.where(label>0.65, 1.0, 0.0)\n        label[2, :, :] = 0.0001\n        label = torch.argmax(label, 0).type(torch.int64)\n        \n        return data, label\n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:00.485751Z","iopub.execute_input":"2023-11-15T14:25:00.486017Z","iopub.status.idle":"2023-11-15T14:25:00.496370Z","shell.execute_reply.started":"2023-11-15T14:25:00.485994Z","shell.execute_reply":"2023-11-15T14:25:00.495356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_train_set_val_set(dataset: Dataset, \n                            train_percent: float = config[\"train_percent\"]\n                           ):\n#     torch.manual_seed(3)\n    n_train = int(len(dataset)*train_percent)\n    n_val = len(dataset) - n_train\n    return random_split(dataset, [n_train, n_val])","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:00.497410Z","iopub.execute_input":"2023-11-15T14:25:00.497706Z","iopub.status.idle":"2023-11-15T14:25:00.507931Z","shell.execute_reply.started":"2023-11-15T14:25:00.497681Z","shell.execute_reply":"2023-11-15T14:25:00.507123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def createDataloader(train_set,\n                     val_set,\n                     batch_size: int = config[\"batch_size\"]\n                    ):\n    train_loader= DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)\n    val_loader = DataLoader(dataset = val_set, batch_size = batch_size, shuffle = False)\n    return train_loader, val_loader","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:00.509022Z","iopub.execute_input":"2023-11-15T14:25:00.509298Z","iopub.status.idle":"2023-11-15T14:25:00.517510Z","shell.execute_reply.started":"2023-11-15T14:25:00.509275Z","shell.execute_reply":"2023-11-15T14:25:00.516713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. Model**","metadata":{}},{"cell_type":"code","source":"def createModel():\n    return smp.Unet(encoder_name=\"resnet34\",encoder_weights=\"imagenet\",in_channels = 3, classes = 3)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:00.520424Z","iopub.execute_input":"2023-11-15T14:25:00.520721Z","iopub.status.idle":"2023-11-15T14:25:00.531204Z","shell.execute_reply.started":"2023-11-15T14:25:00.520686Z","shell.execute_reply":"2023-11-15T14:25:00.530395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Hàm Loss**","metadata":{}},{"cell_type":"code","source":"def createLoss():\n    return nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:00.532224Z","iopub.execute_input":"2023-11-15T14:25:00.532497Z","iopub.status.idle":"2023-11-15T14:25:00.541504Z","shell.execute_reply.started":"2023-11-15T14:25:00.532464Z","shell.execute_reply":"2023-11-15T14:25:00.540672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. Optimizer và learning rate Scheduler**","metadata":{}},{"cell_type":"code","source":"def createOptimizer(model):\n    return torch.optim.Adam(params=model.parameters(), lr=config[\"learning_rate\"])\n\ndef createLRScheduler(optimizer):\n    return torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.6)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:00.542533Z","iopub.execute_input":"2023-11-15T14:25:00.542814Z","iopub.status.idle":"2023-11-15T14:25:00.551786Z","shell.execute_reply.started":"2023-11-15T14:25:00.542791Z","shell.execute_reply":"2023-11-15T14:25:00.551010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Run**","metadata":{}},{"cell_type":"markdown","source":"## Khởi tạo","metadata":{}},{"cell_type":"code","source":"# 1. Tạo dataset và dataloader\nmy_dataset = NeopolypDataset(createPreprocessTransform())\ntrain_set, val_set = split_train_set_val_set(my_dataset)\ntrain_loader, val_loader = createDataloader(train_set, val_set)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:00.552857Z","iopub.execute_input":"2023-11-15T14:25:00.553236Z","iopub.status.idle":"2023-11-15T14:25:00.570368Z","shell.execute_reply.started":"2023-11-15T14:25:00.553182Z","shell.execute_reply":"2023-11-15T14:25:00.569562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_loader.batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:00.571530Z","iopub.execute_input":"2023-11-15T14:25:00.571908Z","iopub.status.idle":"2023-11-15T14:25:00.576834Z","shell.execute_reply.started":"2023-11-15T14:25:00.571847Z","shell.execute_reply":"2023-11-15T14:25:00.575924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. Tạo model\nmodel = createModel().to(device)    ","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:00.578061Z","iopub.execute_input":"2023-11-15T14:25:00.578434Z","iopub.status.idle":"2023-11-15T14:25:04.829813Z","shell.execute_reply.started":"2023-11-15T14:25:00.578401Z","shell.execute_reply":"2023-11-15T14:25:04.828983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3. Tạo loss\nloss_fn = createLoss()","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:04.830973Z","iopub.execute_input":"2023-11-15T14:25:04.831260Z","iopub.status.idle":"2023-11-15T14:25:04.835400Z","shell.execute_reply.started":"2023-11-15T14:25:04.831234Z","shell.execute_reply":"2023-11-15T14:25:04.834598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4. Tạo optimizer\noptimizer = createOptimizer(model)\nlr_scheduler = createLRScheduler(optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:04.836467Z","iopub.execute_input":"2023-11-15T14:25:04.836693Z","iopub.status.idle":"2023-11-15T14:25:04.848602Z","shell.execute_reply.started":"2023-11-15T14:25:04.836673Z","shell.execute_reply":"2023-11-15T14:25:04.847901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load lại model nếu có\nif config[\"load_model\"] != None:\n    checkpoint = torch.load(config[\"load_model\"])\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler_state_dict\"])","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:04.849617Z","iopub.execute_input":"2023-11-15T14:25:04.849952Z","iopub.status.idle":"2023-11-15T14:25:07.114122Z","shell.execute_reply.started":"2023-11-15T14:25:04.849920Z","shell.execute_reply":"2023-11-15T14:25:07.113292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"def train(train_loader, val_loader,\n          model, loss_fn, optimizer, lr_scheduler,\n          epochs: int = config[\"epochs\"]\n         ):\n    \n    for epoch in range(1, epochs + 1):\n        print(f\"Bắt đầu epoch {epoch}\")\n        t1 = time.time()\n        \n        # 1. Train\n        model.train()\n        train_loss = 0\n        for batch_data, batch_label in train_loader:\n            # 1. predict\n            pred = model(batch_data.to(device))\n            \n            # 2. Tính loss\n            loss = loss_fn(pred, batch_label.to(device))\n            train_loss += loss.item()\n            \n            # 3. zero grad\n            optimizer.zero_grad()\n            \n            # 4. Đạo hàm\n            loss.backward()\n            \n            #5. update grad\n            optimizer.step()\n            \n        train_loss /= len(train_loader)\n        lr_scheduler.step()\n        \n        # 2. Val\n        val_loss = 0\n        model.eval()\n        with torch.inference_mode():\n            for batch_data, batch_label in val_loader:\n                pred = model(batch_data.to(device))\n                val_loss += loss_fn(pred, batch_label.to(device))\n            \n        \n        t2 = time.time()\n        \n        # Print\n        print(f\"Train Loss: {train_loss}\")\n        print(f\"Val Loss:{val_loss}\")\n        print(f\"Time: {t2 - t1}\")\n        print(f\"End epoch {epoch}\\n*****************************\")   \n        \n        # wandb\n        wandb.log({\"Train Loss\": train_loss})\n        wandb.log({\"Val Loss\": val_loss})\n               ","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:21:59.026509Z","iopub.execute_input":"2023-11-15T14:21:59.026925Z","iopub.status.idle":"2023-11-15T14:21:59.037362Z","shell.execute_reply.started":"2023-11-15T14:21:59.026885Z","shell.execute_reply":"2023-11-15T14:21:59.036379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(train_loader, val_loader, model, loss_fn, optimizer, lr_scheduler)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:21:59.038530Z","iopub.execute_input":"2023-11-15T14:21:59.038798Z","iopub.status.idle":"2023-11-15T14:22:12.641029Z","shell.execute_reply.started":"2023-11-15T14:21:59.038775Z","shell.execute_reply":"2023-11-15T14:22:12.639865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Infer nộp Test","metadata":{}},{"cell_type":"code","source":"class UNetTestDataClass(Dataset):\n    def __init__(self,  \n                 transform: transforms.Compose,\n                 images_path: str = \"/kaggle/input/bkai-igh-neopolyp/test/test\"):\n        super(UNetTestDataClass, self).__init__()\n        \n        images_list = os.listdir(images_path)\n        images_list = [images_path+\"/\"+i for i in images_list]\n        \n        self.images_list = images_list\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        img_path = self.images_list[index]\n        data = Image.open(img_path)\n        h = data.size[1]\n        w = data.size[0]\n        data = self.transform(data) / 255        \n        return data, img_path, h, w\n    \n    def __len__(self):\n        return len(self.images_list)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:35.112594Z","iopub.execute_input":"2023-11-15T14:25:35.113004Z","iopub.status.idle":"2023-11-15T14:25:35.123032Z","shell.execute_reply.started":"2023-11-15T14:25:35.112970Z","shell.execute_reply":"2023-11-15T14:25:35.121750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = UNetTestDataClass(createPreprocessTransform())\ntest_dataloader = DataLoader(test_dataset,\n                         batch_size = config[\"batch_size\"]\n                        )","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:38.136118Z","iopub.execute_input":"2023-11-15T14:25:38.136916Z","iopub.status.idle":"2023-11-15T14:25:38.152970Z","shell.execute_reply.started":"2023-11-15T14:25:38.136879Z","shell.execute_reply":"2023-11-15T14:25:38.151918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nif not os.path.isdir(\"/kaggle/working/predicted_masks\"):\n    os.mkdir(\"/kaggle/working/predicted_masks\")\nfor _, (img, path, H, W) in enumerate(test_dataloader):\n    a = path\n    b = img\n    h = H\n    w = W\n    \n    with torch.no_grad():\n        predicted_mask = model(b.to(device))\n    for i in range(len(a)):\n        image_id = a[i].split('/')[-1].split('.')[0]\n        filename = image_id + \".png\"\n        mask2img = transforms.Resize((h[i].item(), w[i].item()), interpolation=InterpolationMode.NEAREST)(transforms.ToPILImage()(torch.nn.functional.one_hot(torch.argmax(predicted_mask[i], 0)).permute(2, 0, 1).float()))\n        mask2img.save(os.path.join(\"/kaggle/working/predicted_masks/\", filename))","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:25:40.836433Z","iopub.execute_input":"2023-11-15T14:25:40.836820Z","iopub.status.idle":"2023-11-15T14:26:22.009642Z","shell.execute_reply.started":"2023-11-15T14:25:40.836789Z","shell.execute_reply":"2023-11-15T14:26:22.008575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ndef rle_to_string(runs):\n    return ' '.join(str(x) for x in runs)\n\ndef rle_encode_one_mask(mask):\n    pixels = mask.flatten()\n    pixels[pixels > 0] = 255\n    use_padding = False\n    if pixels[0] or pixels[-1]:\n        use_padding = True\n        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n        pixel_padded[1:-1] = pixels\n        pixels = pixel_padded\n    \n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    if use_padding:\n        rle = rle - 1\n    rle[1::2] = rle[1::2] - rle[:-1:2]\n    return rle_to_string(rle)\n\ndef mask2string(dir):\n    ## mask --> string\n    strings = []\n    ids = []\n    ws, hs = [[] for i in range(2)]\n    for image_id in os.listdir(dir):\n        id = image_id.split('.')[0]\n        path = os.path.join(dir, image_id)\n        print(path)\n        img = cv2.imread(path)[:,:,::-1]\n        h, w = img.shape[0], img.shape[1]\n        for channel in range(2):\n            ws.append(w)\n            hs.append(h)\n            ids.append(f'{id}_{channel}')\n            string = rle_encode_one_mask(img[:,:,channel])\n            strings.append(string)\n    r = {\n        'ids': ids,\n        'strings': strings,\n    }\n    return r\n\n\nMASK_DIR_PATH = '/kaggle/working/predicted_masks' # change this to the path to your output mask folder\ndir = MASK_DIR_PATH\nres = mask2string(dir)\ndf = pd.DataFrame(columns=['Id', 'Expected'])\ndf['Id'] = res['ids']\ndf['Expected'] = res['strings']\ndf.to_csv(r'output.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:26:22.011388Z","iopub.execute_input":"2023-11-15T14:26:22.011755Z","iopub.status.idle":"2023-11-15T14:26:32.578326Z","shell.execute_reply.started":"2023-11-15T14:26:22.011724Z","shell.execute_reply":"2023-11-15T14:26:32.577410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **SAVE model**","metadata":{}},{"cell_type":"code","source":"if not os.path.isdir(config[\"save_folder\"]):\n    os.mkdir(config[\"save_folder\"])\n\ncheckpoint = {\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"lr_scheduler_state_dict\": lr_scheduler.state_dict()\n}\ntorch.save(checkpoint, Path(f\"{config['save_folder']}/save_model.pth\"))","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:12:56.501468Z","iopub.execute_input":"2023-11-15T14:12:56.501860Z","iopub.status.idle":"2023-11-15T14:12:56.961477Z","shell.execute_reply.started":"2023-11-15T14:12:56.501832Z","shell.execute_reply":"2023-11-15T14:12:56.960491Z"},"trusted":true},"execution_count":null,"outputs":[]}]}