{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":30892,"databundleVersionId":2715462,"sourceType":"competition"}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom torchvision.io import read_image\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, random_split, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom torchvision.transforms import ToTensor\nfrom PIL import Image\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision \nfrom torchvision import transforms\nfrom torchinfo import summary\nimport timm\n\n!pip install segmentation-models-pytorch\nimport segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2023-11-16T14:30:36.273036Z","iopub.execute_input":"2023-11-16T14:30:36.273529Z","iopub.status.idle":"2023-11-16T14:30:46.754613Z","shell.execute_reply.started":"2023-11-16T14:30:36.273490Z","shell.execute_reply":"2023-11-16T14:30:46.753658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    import wandb\nexcept:\n    !pip install wandb\nimport wandb\n\nwandb.login(key = \"280aa3837eb27ece3c32ed8e27e3e233d0afdc9c\")\nwandb.init(project=\"Unet_Deep_Learing_Assignment\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1. Class Dataset**","metadata":{}},{"cell_type":"code","source":"class CustomImageDataset(Dataset):\n    def __init__(self, img_dir, label_dir, resize=None, transform=None):\n        self.img_dir = img_dir\n        self.label_dir = label_dir\n        self.resize = resize\n        self.transform = transform\n        self.images = os.listdir(self.img_dir)\n\n    def __len__(self):\n        return len(self.images)\n    \n    def read_mask(self, mask_path):\n        image = cv2.imread(mask_path)\n        image = cv2.resize(image, self.resize)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        # lower boundary RED color range values; Hue (0 - 10)\n        lower1 = np.array([0, 100, 20])\n        upper1 = np.array([10, 255, 255])\n        # upper boundary RED color range values; Hue (160 - 180)\n        lower2 = np.array([160,100,20])\n        upper2 = np.array([179,255,255])\n        lower_mask = cv2.inRange(image, lower1, upper1)\n        upper_mask = cv2.inRange(image, lower2, upper2)\n        \n        red_mask = lower_mask + upper_mask;\n        red_mask[red_mask != 0] = 1\n\n        # boundary GREEN color range values; Hue (36 - 70)\n        green_mask = cv2.inRange(image, (36, 25, 25), (70, 255, 255))\n        green_mask[green_mask != 0] = 2\n\n        full_mask = cv2.bitwise_or(red_mask, green_mask)\n        full_mask = np.expand_dims(full_mask, axis=-1) \n        full_mask = full_mask.astype(np.uint8)\n        return full_mask\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.images[idx])\n        label_path = os.path.join(self.label_dir, self.images[idx])\n        image = cv2.imread(img_path)  # Đọc ảnh dưới dạng BGR\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert sang RGB\n        label = self.read_mask(label_path)  \n        image = cv2.resize(image, self.resize)\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T14:30:47.834676Z","iopub.execute_input":"2023-11-16T14:30:47.835022Z","iopub.status.idle":"2023-11-16T14:30:47.853334Z","shell.execute_reply.started":"2023-11-16T14:30:47.834983Z","shell.execute_reply":"2023-11-16T14:30:47.852364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = []\nTRAIN_DIR = '/kaggle/input/bkai-igh-neopolyp/train/train'\nfor root, dirs, files in os.walk(TRAIN_DIR):\n    # iterate over 1000 images\n    for file in files:\n        # create path\n        path = os.path.join(root,file)\n        # add path to list\n        image_path.append(path)\nlen(image_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T14:30:47.855918Z","iopub.execute_input":"2023-11-16T14:30:47.856260Z","iopub.status.idle":"2023-11-16T14:30:48.386978Z","shell.execute_reply.started":"2023-11-16T14:30:47.856233Z","shell.execute_reply":"2023-11-16T14:30:48.386119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_path = []\nTRAIN_MASK_DIR = '/kaggle/input/bkai-igh-neopolyp/train_gt/train_gt'\nfor root, dirs, files in os.walk(TRAIN_MASK_DIR):\n    #iterate over 1000 masks\n    for file in files:\n        # obtain the path\"\n        path = os.path.join(root,file)\n        # add path to the list\n        mask_path.append(path)\nlen(mask_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T14:30:48.387971Z","iopub.execute_input":"2023-11-16T14:30:48.388250Z","iopub.status.idle":"2023-11-16T14:30:48.947924Z","shell.execute_reply.started":"2023-11-16T14:30:48.388228Z","shell.execute_reply":"2023-11-16T14:30:48.947049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = CustomImageDataset(img_dir= TRAIN_DIR,\n                             label_dir= TRAIN_MASK_DIR,\n                             resize= (256,256),\n                             transform = None)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T14:30:48.949153Z","iopub.execute_input":"2023-11-16T14:30:48.949769Z","iopub.status.idle":"2023-11-16T14:30:48.955420Z","shell.execute_reply.started":"2023-11-16T14:30:48.949733Z","shell.execute_reply":"2023-11-16T14:30:48.954487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 8\nimages_data = []\nlabels_data = []\nfor x,y in dataset:\n    images_data.append(x)\n    labels_data.append(y)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(CustomImageDataset):\n    def __init__(self, data, targets, transform=None):\n        self.data = data\n        self.targets = targets\n        self.transform = transform\n\n    def __getitem__(self, index):\n        image = self.data[index]\n        label = self.targets[index]\n        if self.transform:\n            transformed = self.transform(image=image, mask=label)\n            image = transformed['image'].float()\n            label = transformed['mask'].float()\n            label = label.permute(2, 0, 1)\n        return image, label\n    def __len__(self):\n        return len(self.data)\n\n    \ntrain_transform = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomGamma (gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n    A.RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\nval_transform = A.Compose([\n    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n# Split the dataset into training and validation sets\ntrain_size = int(0.9 * len(images_data))\nval_size = len(images_data) - train_size\ntrain_dataset = CustomDataset(images_data[:train_size], labels_data[:train_size], transform=train_transform)\nval_dataset = CustomDataset(images_data[train_size:], labels_data[train_size:], transform=val_transform)\n# Create data loaders for training and validation\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\nprint(len(train_dataset))\nprint(len(val_dataset))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. Model**","metadata":{}},{"cell_type":"code","source":"model = smp.UnetPlusPlus(\n    encoder_name=\"resnet34\",        \n    encoder_weights=\"imagenet\",     \n    in_channels=3,                  \n    classes=3     \n)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T14:31:09.582530Z","iopub.execute_input":"2023-11-16T14:31:09.582931Z","iopub.status.idle":"2023-11-16T14:31:12.560747Z","shell.execute_reply.started":"2023-11-16T14:31:09.582885Z","shell.execute_reply":"2023-11-16T14:31:12.559922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Learning rate and Optimizer**","metadata":{}},{"cell_type":"code","source":"learning_rate = 0.0001\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T14:31:54.651830Z","iopub.execute_input":"2023-11-16T14:31:54.652103Z","iopub.status.idle":"2023-11-16T14:31:54.658030Z","shell.execute_reply.started":"2023-11-16T14:31:54.652079Z","shell.execute_reply":"2023-11-16T14:31:54.657183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_dict= {0: (0, 0, 0),\n             1: (255, 0, 0),\n             2: (0, 255, 0)}\ndef mask_to_rgb(mask, color_dict):\n    output = np.zeros((mask.shape[0], mask.shape[1], 3))\n#     print(output.shape)\n    for k in color_dict.keys():\n        output[mask==k] = color_dict[k]\n\n    return np.uint8(output)    ","metadata":{"execution":{"iopub.status.busy":"2023-11-16T14:31:54.659273Z","iopub.execute_input":"2023-11-16T14:31:54.659522Z","iopub.status.idle":"2023-11-16T14:31:54.669754Z","shell.execute_reply.started":"2023-11-16T14:31:54.659500Z","shell.execute_reply":"2023-11-16T14:31:54.669076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Train**","metadata":{}},{"cell_type":"code","source":"# Set the number of training epochs\nnum_epochs = 200\n\n# Move the model to the device (e.g., GPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ncriterion = nn.CrossEntropyLoss()\nbest_val_loss = 999\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        # Forward pass\n        labels = labels.squeeze(dim=1).long()\n        outputs = model(images)\n        # Compute the loss\n    \n        loss = criterion(outputs, labels)\n        train_loss += loss.item()\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    train_loss /= len(train_loader)\n\n    # Perform validation\n    model.eval()\n    with torch.no_grad():\n        val_loss = 0\n        for images, labels in val_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            labels = labels.squeeze(dim=1).long()\n            \n            # Forward pass\n            outputs = model(images)\n#             print(outputs.shape)\n\n            val_loss += criterion(outputs.float(),labels.long()).item()\n        val_loss /= len(val_loader)\n    # Print the loss for this epoch\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {val_loss/len(val_loader):.10f}\")\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        checkpoint = { \n            'epoch': epoch,\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n            'loss': val_loss,\n        }\n        save_path = f'colorization_model.pth'\n        torch.save(checkpoint, save_path)\n        print('Save new model')\n    wandb.log({\n        \"Train Loss\": train_loss,\n        \"Val_Loss\": val_loss\n    })","metadata":{"execution":{"iopub.status.busy":"2023-11-16T14:31:54.670892Z","iopub.execute_input":"2023-11-16T14:31:54.671215Z","iopub.status.idle":"2023-11-16T14:34:11.211494Z","shell.execute_reply.started":"2023-11-16T14:31:54.671191Z","shell.execute_reply":"2023-11-16T14:34:11.210109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Eval Test**","metadata":{}},{"cell_type":"code","source":"! mkdir predicted_mask","metadata":{"execution":{"iopub.status.busy":"2023-11-16T14:36:41.513830Z","iopub.execute_input":"2023-11-16T14:36:41.514534Z","iopub.status.idle":"2023-11-16T14:36:42.547061Z","shell.execute_reply.started":"2023-11-16T14:36:41.514495Z","shell.execute_reply":"2023-11-16T14:36:42.545932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nfor i in os.listdir(\"/kaggle/input/bkai-igh-neopolyp/test/test\"):\n    img_path = os.path.join(\"/kaggle/input/bkai-igh-neopolyp/test/test\", i)\n    ori_img = cv2.imread(img_path)\n    ori_img = cv2.cvtColor(ori_img, cv2.COLOR_BGR2RGB)\n    ori_w = ori_img.shape[0]\n    ori_h = ori_img.shape[1]\n    img = cv2.resize(ori_img, (256, 256))\n    transformed = val_transform(image=img)\n    input_img = transformed[\"image\"]\n    input_img = input_img.unsqueeze(0).to(device)\n    with torch.no_grad():\n        output_mask = model.forward(input_img).squeeze(0).cpu().numpy().transpose(1,2,0)\n    mask = cv2.resize(output_mask, (ori_h, ori_w))\n    mask = np.argmax(mask, axis=2)\n    new_rgb_mask = np.zeros((*mask.shape, 3)).astype(np.uint8)\n    mask_rgb = mask_to_rgb(mask, color_dict)\n    cv2.imwrite(\"predicted_mask/{}\".format(i), mask_rgb)\n\n     ","metadata":{"execution":{"iopub.status.busy":"2023-11-16T14:36:45.333502Z","iopub.execute_input":"2023-11-16T14:36:45.334795Z","iopub.status.idle":"2023-11-16T14:37:13.107736Z","shell.execute_reply.started":"2023-11-16T14:36:45.334753Z","shell.execute_reply":"2023-11-16T14:37:13.106819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport os\n\ndef rle_to_string(runs):\n    return ' '.join(str(x) for x in runs)\n\ndef rle_encode_one_mask(mask):\n    pixels = mask.flatten()\n    pixels[pixels > 225] = 255\n    pixels[pixels <= 225] = 0\n    use_padding = False\n    if pixels[0] or pixels[-1]:\n        use_padding = True\n        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n        pixel_padded[1:-1] = pixels\n        pixels = pixel_padded\n    \n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    if use_padding:\n        rle = rle - 1\n    rle[1::2] = rle[1::2] - rle[:-1:2]\n    return rle_to_string(rle)\n\ndef rle2mask(mask_rle, shape=(3,3)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (width,height) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T\n\ndef mask2string(dir):\n    ## mask --> string\n    strings = []\n    ids = []\n    ws, hs = [[] for i in range(2)]\n    for image_id in os.listdir(dir):\n        id = image_id.split('.')[0]\n        path = os.path.join(dir, image_id)\n        print(path)\n        img = cv2.imread(path)[:,:,::-1]\n        h, w = img.shape[0], img.shape[1]\n        for channel in range(2):\n            ws.append(w)\n            hs.append(h)\n            ids.append(f'{id}_{channel}')\n            string = rle_encode_one_mask(img[:,:,channel])\n            strings.append(string)\n    r = {\n        'ids': ids,\n        'strings': strings,\n    }\n    return r\n\n\nMASK_DIR_PATH = '/kaggle/working/predicted_mask' # change this to the path to your output mask folder\ndir = MASK_DIR_PATH\nres = mask2string(dir)\ndf = pd.DataFrame(columns=['Id', 'Expected'])\ndf['Id'] = res['ids']\ndf['Expected'] = res['strings']\n\ndf.to_csv(r'output.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T14:39:29.414063Z","iopub.execute_input":"2023-11-16T14:39:29.414533Z","iopub.status.idle":"2023-11-16T14:39:32.798352Z","shell.execute_reply.started":"2023-11-16T14:39:29.414497Z","shell.execute_reply":"2023-11-16T14:39:32.797196Z"},"trusted":true},"execution_count":null,"outputs":[]}]}